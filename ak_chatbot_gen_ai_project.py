# -*- coding: utf-8 -*-
"""AK_CHATBOT_GEN_AI_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14IwrjABmarQ2bGIlal59YGT7nSr0r8Zw
"""

!pip install langchain_groq langchain_core langchain_community

from langchain_groq import ChatGroq
llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile",
)
result = llm.invoke("who is lord jesus christ?")
print(result.content)

!pip install pypdf

!pip install chromadb

!pip install sentence_transformers

#create_vector_db()

!pip install gradio

!pip install -U langchain-community

!pip install langchain-groq

!pip install pypdf

!pip install chromadb

!pip install -U transformers==4.41.0

import transformers
print(transformers.__version__)

!pip install -q transformers==4.41.1 sentence-transformers==2.6.1 \
            gradio langchain langchain-groq chromadb unstructured pdfminer.six pypdf

pip install -U "sentence-transformers>=2.7.0"

pip install -U "transformers==4.38.2"

pip install -U "sentence-transformers>=2.7.0"

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  OPTIONAL one-time installs (â‰ˆ1â€“2 min on Colab)          â•‘
# â•‘----------------------------------------------------------â•‘
# !pip install -q gradio transformers==4.38.2 sentencepiece
# !pip install -q pandas matplotlib pillow
# !pip install -q langchain langchain-groq chromadb pypdf
# !pip install -q unstructured
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os, re, time, base64, textwrap, warnings, pathlib
import gradio as gr
from transformers               import pipeline
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter    import RecursiveCharacterTextSplitter
from langchain.vectorstores     import Chroma
from langchain.embeddings       import HuggingFaceBgeEmbeddings
from langchain.memory           import ConversationBufferMemory
from langchain.chains           import ConversationalRetrievalChain
from langchain.prompts          import PromptTemplate
from langchain_groq             import ChatGroq
from groq                       import RateLimitError

warnings.filterwarnings("ignore")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API_KEY = "YOUR_API_KEY"
MODEL   = "llama-3.3-70b-versatile"
DB_PATH = "./chromadb"
LOG     = "./chat_transcript.txt"
PFP     = "profile.jpg"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Pipelines â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
memory     = ConversationBufferMemory(memory_key="chat_history",
                                      return_messages=True,
                                      output_key="answer")
summariser = pipeline("summarization", model="t5-small",
                      truncation=True, max_length=256)
asr        = pipeline("automatic-speech-recognition",
                      model="openai/whisper-tiny", device=-1)

vector_db, qa_chain = None, None
pdf_text            = ""
cooldown_until      = 0.0
ask_me_later_queue  = []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def new_llm():
    return ChatGroq(temperature=0,
                    groq_api_key=API_KEY,
                    model_name=MODEL,
                    max_tokens=256)

def load_or_build_db(chunks=None):
    embed = HuggingFaceBgeEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    if chunks:
        db = Chroma.from_documents(chunks, embed, persist_directory=DB_PATH)
        db.persist(); return db
    if os.path.exists(DB_PATH):
        return Chroma(persist_directory=DB_PATH, embedding_function=embed)
    return None

def make_chain(vdb):
    prompt = PromptTemplate(
        template=("You are **GenAI Analyst Copilot**. Respond concisely.\n\n"
                  "{context}\nUser: {question}\nAssistant:"),
        input_variables=["context", "question"])
    return ConversationalRetrievalChain.from_llm(
        llm=new_llm(),
        retriever=vdb.as_retriever(search_kwargs={"k": 3}),
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt},
        return_source_documents=False,
        output_key="answer")

vector_db = load_or_build_db()
qa_chain  = make_chain(vector_db) if vector_db else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ File upload / processing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def handle_upload(files, progress=gr.Progress(track_tqdm=True)):
    global vector_db, qa_chain, pdf_text
    docs, summaries, pdf_text = [], [], ""
    for f in progress.tqdm(files or [], desc="Parsing PDFs"):
        name  = os.path.basename(f.name)
        pages = PyPDFLoader(f.name).load()
        docs += pages
        chunk = " ".join(p.page_content for p in pages)[:1000]
        pdf_text += " ".join(p.page_content for p in pages) + " "
        summ = summariser(chunk, min_length=40, max_length=120,
                          truncation=True, do_sample=False)[0]["summary_text"]
        summaries.append(f"### **{name}**\n{textwrap.fill(summ, 90)}")
    if docs:
        chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\
                 .split_documents(docs)
        vector_db = load_or_build_db(chunks)
        qa_chain  = make_chain(vector_db)
    return (f"âœ… Processed {len(docs)} PDF(s).",
            "\n\n---\n\n".join(summaries) or "No summaries.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Voice â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def transcribe_and_chat(audio):
    if audio is None:
        return "No audio.", ""
    user = asr(audio)["text"].strip()
    return f"**You said:** {user}", respond(user, "")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Quiz generator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def gen_questions():
    if not pdf_text:
        return "âš ï¸ Upload at least one PDF first."
    prompt = f"Generate 5 short quiz questions (no answers) about this document:\n\n{pdf_text[:4000]}"
    try:
        return new_llm().invoke(prompt).content
    except:
        return "Generation unavailable."

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Chat logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _wait_secs(msg):
    m = re.search(r"try again in (\d+)m([\d\.]+)s", msg) or \
        re.search(r"try again in ([\d\.]+)s", msg)
    if not m:
        return 300
    return int(float(m.group(1))*60 + float(m.group(2))) if len(m.groups()) == 2 \
           else int(float(m.group(1)))

def respond(user, history):
    global cooldown_until
    if time.time() < cooldown_until:
        ask_me_later_queue.append(user)
        yield "ğŸ•’ Added to queue â€“ I'll answer when quota resets."
        return
    if qa_chain is None:
        yield "ğŸ“‚ Upload a PDF first."; return
    yield "ğŸ’¬ Bot is typing â€¦"
    try:
        ans = qa_chain({"question": user})["answer"]
        pathlib.Path(LOG).write_text(
            pathlib.Path(LOG).read_text("") + f"User:{user}\nBot:{ans}\n\n")
        yield ans
    except RateLimitError as e:
        cooldown_until = time.time() + _wait_secs(str(e))
        ask_me_later_queue.append(user)
        yield "âš ï¸ Groq quota hit â€“ queued your question."

def serve_queue_btn():
    if time.time() < cooldown_until or not ask_me_later_queue:
        return "â³ Nothing to serve yet."
    q   = ask_me_later_queue.pop(0)
    ans = qa_chain({"question": q})["answer"]
    return f"**Queued Q:** {q}\n\n{ans}"

def get_transcript():
    if not os.path.exists(LOG):
        pathlib.Path(LOG).write_text("Chat started\n")
    return LOG

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CSS for solid dark theme â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
extra_css = """
<style>
@keyframes sparkle{0%{transform:translateY(0) scale(1);opacity:.8}
                   100%{transform:translateY(-80vh) scale(.6);opacity:0}}

html, body, .gradio-container{
  background:#0b0c10 !important;   /* deep dark background */
  color:#ffffff;
  font-family:'Times New Roman',serif;
  text-align:center;
}

.gr-block, .gr-box{background:#1f1f1f !important;border:none !important}

.gr-chat-message          {border:1px solid #333;border-radius:14px}
.gr-chat-message.from-user{background:#222}
.gr-chat-message.from-bot {background:#1a1a1a}

input:focus,textarea:focus,button:focus{
  outline:none;box-shadow:0 0 0 3px #444,0 0 8px #222}

.pfp{position:fixed;top:18px;right:26px;text-align:center;z-index:999}
.pfp img{width:64px;height:64px;border-radius:50%;object-fit:cover;
         border:2px solid #fff;box-shadow:0 0 6px rgba(0,0,0,.25)}
.pfp-name{font-size:14px;margin-top:4px;color:#ffffff}

.sparkle{position:fixed;width:6px;height:6px;border-radius:50%;background:#fff;
         box-shadow:0 0 6px #fff;animation:sparkle 10s linear infinite;
         pointer-events:none}
</style>
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sparkle markup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sparkles_html = "".join(
    f'<div class="sparkle" style="left:{i}%"></div>' for i in range(2, 100, 4)
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Optional JS (confetti & sparkle cursor) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
js_code = """
document.addEventListener('mousemove', e=>{
  const d=document.createElement('div');
  d.className='sparkle';
  d.style.left=e.pageX-3+'px';
  d.style.top =e.pageY-3+'px';
  document.body.appendChild(d);
  setTimeout(()=>d.remove(),800);
});
window.addEventListener('message', e=>{
  if(e.data==='confetti'){
    import('https://cdn.jsdelivr.net/npm/canvas-confetti@1.6.0/dist/confetti.module.mjs')
      .then(m=>m.default({origin:{y:0.2},spread:90,particleCount:120}));
  }
});
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Avatar HTML (if file exists) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pfp64 = base64.b64encode(open(PFP, "rb").read()).decode() if os.path.exists(PFP) else ""
pfp_html = (f'<div class="pfp"><img src="data:image/jpeg;base64,{pfp64}"/>'
            '<div class="pfp-name">Akshara</div></div>') if pfp64 else ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Build Gradio UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
theme = gr.themes.Soft()

with gr.Blocks(theme=theme) as demo:
    gr.HTML(extra_css + sparkles_html + pfp_html)

    part = ("morning" if 5 <= time.localtime().tm_hour < 12 else
            "afternoon" if 12 <= time.localtime().tm_hour < 18 else
            "evening")
    gr.Markdown(f"**Good {part}, Akshara <3**")
    gr.Markdown("## InsightBot with GenAI")

    up = gr.File(label="ğŸ“ Upload PDF File(s)", file_types=[".pdf"], file_count="multiple")
    st = gr.Textbox(label="Status", interactive=False)
    sm = gr.Markdown()
    up.change(handle_upload, up, [st, sm])\
      .then(None, None, None, js="()=>window.postMessage('confetti','*')")

    gr.Markdown('<h3 id="agenda">Whatâ€™s on your agenda today ?</h3>')

    gr.ChatInterface(respond)

    gr.Markdown("### â“ Generate Quiz Questions")
    q_btn, q_out = gr.Button("Generate Questions"), gr.Markdown()
    q_btn.click(gen_questions, None, q_out)

    gr.Markdown("### ğŸ™ï¸ Voice Chat")
    aud = gr.Audio(type="filepath", label="Record then release")
    spk, rep = gr.Markdown(), gr.Markdown()
    aud.change(transcribe_and_chat, aud, [spk, rep])

    gr.Markdown("### ğŸ”„ Serve Queued Answers")
    refresh = gr.Button("Refresh queued answers")
    refresh.click(serve_queue_btn, None, q_out)

    gr.Markdown("### ğŸ“„ Download Chat Transcript")
    dl_btn, dl_file = gr.Button("Download transcript ğŸ“¥"), gr.File(interactive=False)
    dl_btn.click(get_transcript, None, dl_file)

    gr.Markdown("---<br><small>GenAI Copilot ChatBot â€“ made by <strong>Akshara Avinash Sarode</strong></small>")
    demo.load(None, None, None, js=js_code)

demo.launch(share=True)